text(0.6, 0.3, paste("AUC =", round(auc(roc_object), 2)), col = "#1c61b6")
View(test_df_PCE)
View(test_df_PCE)
print(pred_train_PCE)
which(pred_train_PCE == 1)
which(pred_test_PCE == 1)
#
pred_test_PCE <- predict(logr_model_PCE, newdata = test_df_PCE, type="response")
# create ROC object
roc_object <- roc(test_df_PCE$NegOutcome, pred_test_PCE)
# calculate area under curve
auc(roc_object)
plot(roc_object, main = "ROC Curve", col = "#1c61b6", lwd = 2)
text(0.6, 0.3, paste("AUC =", round(auc(roc_object), 2)), col = "#1c61b6")
# Training set results
pred_train_SDOH <- predict(logr_model_SDOH, newdata = train_df_SDOH, type="response")
logr_model_SDOH <- glm(NegOutcome ~., family=binomial, data=train_df_SDOH)
summary(logr_model_SDOH)
# Training set results
pred_train_SDOH <- predict(logr_model_SDOH, newdata = train_df_SDOH, type="response")
# Change probabilities to 0, 1
pred_train_SDOH <- ifelse(pred_train_SDOH > 0.5, 1, 0)
# Test set results
pred_test_SDOH <- predict(logr_model_SDOH, newdata = test_df_SDOH, type="response")
# Change probabilities to 0, 1
pred_test_SDOH <- ifelse(pred_test_SDOH > 0.5, 1, 0)
# Converting to factors
train_actual_SDOH <- factor(train_df_SDOH$NegOutcome, levels = c(0, 1))
test_actual_SDOH <- factor(test_df_SDOH$NegOutcome, levels = c(0, 1))
train_pred_SDOH <- factor(pred_train_SDOH, levels = c(0, 1))
test_pred_SDOH <- factor(pred_test_SDOH, levels = c(0, 1))
# Building Confusion Matrices
results_SDOH_train <- confusionMatrix(train_actual_SDOH, train_pred_SDOH, positive = "1")
print(results_SDOH_train)
results_SDOH_test <- confusionMatrix(test_actual_SDOH, test_pred_SDOH, positive = "1")
print(results_SDOH_test)
pred_test_SDOH <- predict(logr_model_SDOH, newdata = test_df_SDOH, type="response")
# create ROC object
roc_object <- roc(test_df_SDOH$NegOutcome, pred_test_SDOH)
# calculate area under curve
auc(roc_object)
plot(roc_object, main = "ROC Curve", col = "#1c61b6", lwd = 2)
text(0.6, 0.3, paste("AUC =", round(auc(roc_object), 2)), col = "#1c61b6")
# Load in Libraries
library(CVrisk)
library(ggplot2)
library(dplyr)
library(haven)
library(tidyverse)
library(broom)
library(ggplot2)
library(caret)
library(tidymodels)
library(pROC)
# For Training
library(OpenML)
library(DT)
library(randomForest)
knitr::opts_knit$set(root.dir = '/Users/noreenmayat/Desktop/Senior Project')
## Visualizations for logistic regression and then different model (important variables, summary statistics, scores)
# EDA
# plots for continuous variables / covariances
# table 1 - avg value and std for each covariant; split between negative vs. non-negative outcomes in 2 columns, each row is a different covariant -- R table 1 package
# box plots
# distribution of the ASCVD score for negative vs non-negative outcomes -- observe density
# Load in Data
train_df <- read.csv("Cleaned Data/imputed_train.csv")
test_df <- read.csv("Cleaned Data/imputed_test.csv")
full_df <- read.csv("Cleaned Data/imputed_full.csv")
# Re-code as binaries for ML
train_df$Race1 <- ifelse(train_df$Race1 %in% c("white", "other"), 0, 1)
train_df$Sex <- ifelse(train_df$Sex == "male", 0, 1)
train_df$X <- NULL
test_df$Race1 <- ifelse(test_df$Race1 %in% c("white", "other"), 0, 1)
test_df$Sex <- ifelse(test_df$Sex == "male", 0, 1)
test_df$X <- NULL
full_df$Race1 <- ifelse(full_df$Race1 %in% c("white", "other"), 0, 1)
full_df$Sex <- ifelse(full_df$Sex == "male", 0, 1)
full_df$X <- NULL
# Compare Negative Outcomes by FastFood categories
barplot(table(full_df$NegOutcome, full_df$FastFood),
beside = T,
legend.text = T,
xlab = "Fast Food",
ylab = "Frequency",
main = "Negative CVD Outcome by Fast Food")
# Add a box around the plot
box()
# maybe use proportion instead of frequency
# boxplots -- for each variable for each category
# PCE
sum(is.na(train_df))
sum(is.na(test_df))
# Removing NAs
train_df <- na.omit(train_df)
test_df <- na.omit(test_df)
# Changing to factors
train_df$Race2 <- factor(train_df$Race2)
test_df$Race2 <- factor(test_df$Race2)
train_df <- train_df %>%
mutate(across(all_of(c('Ann_FamIncome', 'Ann_HHIncome', 'FastFood', 'BalancedMeals', 'LowCost', 'FoodSecurity')), ~factor(.x, levels = as.character(0:max(.x, na.rm = TRUE)), ordered = TRUE)))
test_df <- test_df %>%
mutate(across(all_of(c('Ann_FamIncome', 'Ann_HHIncome', 'FastFood', 'BalancedMeals', 'LowCost', 'FoodSecurity')), ~factor(.x, levels = as.character(0:max(.x, na.rm = TRUE)), ordered = TRUE)))
blueprint <- recipe(NegOutcome ~ ., data = train_df) %>%
step_center(all_of(c('Age', 'PovertyRatio', 'HDLChol', 'TotalChol', 'AvgSysBP'))) %>%
step_scale(all_of(c('Age', 'PovertyRatio', 'HDLChol', 'TotalChol', 'AvgSysBP')))
## Estimating blueprint parameters
blueprint_prep <- prep(blueprint, training = train_df)
## Transforming data
transformed_train <- bake(blueprint_prep, new_data = train_df)
transformed_test <- bake(blueprint_prep, new_data = test_df)
# Subset Appropriate Columns-- without SDOH, just PCE variables
train_df_PCE <- transformed_train[c('Age', 'Sex', 'Race1', 'HDLChol', 'TotalChol', 'AvgSysBP', 'BPMed', 'Diabetes', 'Smoking', 'NegOutcome')]
test_df_PCE <- transformed_test[c('Age', 'Sex', 'Race1', 'HDLChol', 'TotalChol', 'AvgSysBP', 'BPMed', 'Diabetes', 'Smoking', 'NegOutcome')]
# Subset Appropriate Columns-- with SDOH, and more stratified race variables
train_df_SDOH <- transformed_train[c('Age', 'Sex', 'Race2', 'HDLChol', 'TotalChol', 'AvgSysBP', 'BPMed', 'Diabetes', 'Smoking', 'Ann_FamIncome', 'Ann_HHIncome', 'PovertyRatio', 'FastFood', 'BalancedMeals', 'LowCost', 'FoodSecurity', 'NegOutcome')]
test_df_SDOH <- transformed_test[c('Age', 'Sex', 'Race2', 'HDLChol', 'TotalChol', 'AvgSysBP', 'BPMed', 'Diabetes', 'Smoking', 'Ann_FamIncome', 'Ann_HHIncome', 'PovertyRatio', 'FastFood', 'BalancedMeals', 'LowCost', 'FoodSecurity', 'NegOutcome')]
logr_model_PCE <- glm(NegOutcome ~., family=binomial, data=train_df_PCE)
summary(logr_model_PCE)
# Training set results
pred_train_PCE <- predict(logr_model_PCE, newdata = train_df_PCE, type="response")
# Change probabilities to 0, 1
pred_train_PCE <- ifelse(pred_train_PCE > 0.5, 1, 0)
# Test set results
pred_test_PCE <- predict(logr_model_PCE, newdata = test_df_PCE, type="response")
# Change probabilities to 0, 1
pred_test_PCE <- ifelse(pred_test_PCE > 0.5, 1, 0)
# Converting to factors
train_actual_PCE <- factor(train_df_PCE$NegOutcome, levels = c(0, 1))
test_actual_PCE <- factor(test_df_PCE$NegOutcome, levels = c(0, 1))
train_pred_PCE <- factor(pred_train_PCE, levels = c(0, 1))
test_pred_PCE <- factor(pred_test_PCE, levels = c(0, 1))
# Building Confusion Matrices
results_PCE_train <- confusionMatrix(train_actual_PCE, train_pred_PCE, positive = "1")
print(results_PCE_train)
results_PCE_test <- confusionMatrix(test_actual_PCE, test_pred_PCE, positive = "1")
print(results_PCE_test)
pred_test_PCE <- predict(logr_model_PCE, newdata = test_df_PCE, type="response")
# create ROC object
roc_object <- roc(test_df_PCE$NegOutcome, pred_test_PCE)
# calculate area under curve
auc(roc_object)
plot(roc_object, main = "ROC Curve", col = "#1c61b6", lwd = 2, print.thres=TRUE)
text(0.6, 0.3, paste("AUC =", round(auc(roc_object), 2)), col = "#1c61b6")
# code for optimal threshold value
logr_model_SDOH <- glm(NegOutcome ~., family=binomial, data=train_df_SDOH)
summary(logr_model_SDOH)
# Training set results
pred_train_SDOH <- predict(logr_model_SDOH, newdata = train_df_SDOH, type="response")
# Change probabilities to 0, 1
pred_train_SDOH <- ifelse(pred_train_SDOH > 0.5, 1, 0)
# Test set results
pred_test_SDOH <- predict(logr_model_SDOH, newdata = test_df_SDOH, type="response")
# Change probabilities to 0, 1
pred_test_SDOH <- ifelse(pred_test_SDOH > 0.5, 1, 0)
# Converting to factors
train_actual_SDOH <- factor(train_df_SDOH$NegOutcome, levels = c(0, 1))
test_actual_SDOH <- factor(test_df_SDOH$NegOutcome, levels = c(0, 1))
train_pred_SDOH <- factor(pred_train_SDOH, levels = c(0, 1))
test_pred_SDOH <- factor(pred_test_SDOH, levels = c(0, 1))
# Building Confusion Matrices
results_SDOH_train <- confusionMatrix(train_actual_SDOH, train_pred_SDOH, positive = "1")
print(results_SDOH_train)
results_SDOH_test <- confusionMatrix(test_actual_SDOH, test_pred_SDOH, positive = "1")
print(results_SDOH_test)
pred_test_SDOH <- predict(logr_model_SDOH, newdata = test_df_SDOH, type="response")
# create ROC object
roc_object <- roc(test_df_SDOH$NegOutcome, pred_test_SDOH)
# calculate area under curve
auc(roc_object)
plot(roc_object, main = "ROC Curve", col = "#1c61b6", lwd = 2, print.thres=TRUE)
text(0.6, 0.3, paste("AUC =", round(auc(roc_object), 2)), col = "#1c61b6")
# code for optimal value -- fixed 80% sensitivity, how good is specificity
# test for different thresholds/lower thresholds
# Training set results
pred_train_PCE <- predict(logr_model_PCE, newdata = train_df_PCE, type="response")
# Change probabilities to 0, 1
pred_train_PCE <- ifelse(pred_train_PCE > 0.5, 1, 0)
# Test set results
pred_test_PCE <- predict(logr_model_PCE, newdata = test_df_PCE, type="response")
# Change probabilities to 0, 1
pred_test_PCE <- ifelse(pred_test_PCE > 0.06, 1, 0)
# Converting to factors
train_actual_PCE <- factor(train_df_PCE$NegOutcome, levels = c(0, 1))
test_actual_PCE <- factor(test_df_PCE$NegOutcome, levels = c(0, 1))
train_pred_PCE <- factor(pred_train_PCE, levels = c(0, 1))
test_pred_PCE <- factor(pred_test_PCE, levels = c(0, 1))
# Building Confusion Matrices
results_PCE_train <- confusionMatrix(train_actual_PCE, train_pred_PCE, positive = "1")
print(results_PCE_train)
results_PCE_test <- confusionMatrix(test_actual_PCE, test_pred_PCE, positive = "1")
print(results_PCE_test)
# Training set results
pred_train_PCE <- predict(logr_model_PCE, newdata = train_df_PCE, type="response")
# Change probabilities to 0, 1
pred_train_PCE <- ifelse(pred_train_PCE > 0.5, 1, 0)
# Test set results
pred_test_PCE <- predict(logr_model_PCE, newdata = test_df_PCE, type="response")
# Change probabilities to 0, 1
pred_test_PCE <- ifelse(pred_test_PCE > 0.6, 1, 0)
# Converting to factors
train_actual_PCE <- factor(train_df_PCE$NegOutcome, levels = c(0, 1))
test_actual_PCE <- factor(test_df_PCE$NegOutcome, levels = c(0, 1))
train_pred_PCE <- factor(pred_train_PCE, levels = c(0, 1))
test_pred_PCE <- factor(pred_test_PCE, levels = c(0, 1))
# Building Confusion Matrices
results_PCE_train <- confusionMatrix(train_actual_PCE, train_pred_PCE, positive = "1")
print(results_PCE_train)
results_PCE_test <- confusionMatrix(test_actual_PCE, test_pred_PCE, positive = "1")
print(results_PCE_test)
# Training set results
pred_train_PCE <- predict(logr_model_PCE, newdata = train_df_PCE, type="response")
# Change probabilities to 0, 1
pred_train_PCE <- ifelse(pred_train_PCE > 0.5, 1, 0)
# Test set results
pred_test_PCE <- predict(logr_model_PCE, newdata = test_df_PCE, type="response")
# Change probabilities to 0, 1
pred_test_PCE <- ifelse(pred_test_PCE > 0.65, 1, 0)
# Converting to factors
train_actual_PCE <- factor(train_df_PCE$NegOutcome, levels = c(0, 1))
test_actual_PCE <- factor(test_df_PCE$NegOutcome, levels = c(0, 1))
train_pred_PCE <- factor(pred_train_PCE, levels = c(0, 1))
test_pred_PCE <- factor(pred_test_PCE, levels = c(0, 1))
# Building Confusion Matrices
results_PCE_train <- confusionMatrix(train_actual_PCE, train_pred_PCE, positive = "1")
print(results_PCE_train)
results_PCE_test <- confusionMatrix(test_actual_PCE, test_pred_PCE, positive = "1")
print(results_PCE_test)
pred_test_PCE <- predict(logr_model_PCE, newdata = test_df_PCE, type="response")
# create ROC object
roc_object <- roc(test_df_PCE$NegOutcome, pred_test_PCE)
# calculate area under curve
auc(roc_object)
plot(roc_object, main = "ROC Curve", col = "#1c61b6", lwd = 2, print.thres=TRUE)
text(0.6, 0.3, paste("AUC =", round(auc(roc_object), 2)), col = "#1c61b6")
# code for optimal threshold value
logr_model_SDOH <- glm(NegOutcome ~., family=binomial, data=train_df_SDOH)
summary(logr_model_SDOH)
# Training set results
pred_train_SDOH <- predict(logr_model_SDOH, newdata = train_df_SDOH, type="response")
# Change probabilities to 0, 1
pred_train_SDOH <- ifelse(pred_train_SDOH > 0.5, 1, 0)
# Test set results
pred_test_SDOH <- predict(logr_model_SDOH, newdata = test_df_SDOH, type="response")
# Change probabilities to 0, 1
pred_test_SDOH <- ifelse(pred_test_SDOH > 0.5, 1, 0)
# Converting to factors
train_actual_SDOH <- factor(train_df_SDOH$NegOutcome, levels = c(0, 1))
test_actual_SDOH <- factor(test_df_SDOH$NegOutcome, levels = c(0, 1))
train_pred_SDOH <- factor(pred_train_SDOH, levels = c(0, 1))
test_pred_SDOH <- factor(pred_test_SDOH, levels = c(0, 1))
# Building Confusion Matrices
results_SDOH_train <- confusionMatrix(train_actual_SDOH, train_pred_SDOH, positive = "1")
print(results_SDOH_train)
results_SDOH_test <- confusionMatrix(test_actual_SDOH, test_pred_SDOH, positive = "1")
print(results_SDOH_test)
pred_test_SDOH <- predict(logr_model_SDOH, newdata = test_df_SDOH, type="response")
# create ROC object
roc_object <- roc(test_df_SDOH$NegOutcome, pred_test_SDOH)
# calculate area under curve
auc(roc_object)
plot(roc_object, main = "ROC Curve", col = "#1c61b6", lwd = 2, print.thres=TRUE)
text(0.6, 0.3, paste("AUC =", round(auc(roc_object), 2)), col = "#1c61b6")
# code for optimal value -- fixed 80% sensitivity, how good is specificity
# test for different thresholds/lower thresholds
# Training set results
pred_train_PCE <- predict(logr_model_PCE, newdata = train_df_PCE, type="response")
# Change probabilities to 0, 1
pred_train_PCE <- ifelse(pred_train_PCE > 0.65, 1, 0)
# Test set results
pred_test_PCE <- predict(logr_model_PCE, newdata = test_df_PCE, type="response")
# Change probabilities to 0, 1
pred_test_PCE <- ifelse(pred_test_PCE > 0.65, 1, 0)
# Converting to factors
train_actual_PCE <- factor(train_df_PCE$NegOutcome, levels = c(0, 1))
test_actual_PCE <- factor(test_df_PCE$NegOutcome, levels = c(0, 1))
train_pred_PCE <- factor(pred_train_PCE, levels = c(0, 1))
test_pred_PCE <- factor(pred_test_PCE, levels = c(0, 1))
# Building Confusion Matrices
results_PCE_train <- confusionMatrix(train_actual_PCE, train_pred_PCE, positive = "1")
print(results_PCE_train)
results_PCE_test <- confusionMatrix(test_actual_PCE, test_pred_PCE, positive = "1")
print(results_PCE_test)
pred_test_PCE <- predict(logr_model_PCE, newdata = test_df_PCE, type="response")
# create ROC object
roc_object <- roc(test_df_PCE$NegOutcome, pred_test_PCE)
# calculate area under curve
auc(roc_object)
plot(roc_object, main = "ROC Curve", col = "#1c61b6", lwd = 2, print.thres=TRUE)
text(0.6, 0.3, paste("AUC =", round(auc(roc_object), 2)), col = "#1c61b6")
# code for optimal threshold value
logr_model_SDOH <- glm(NegOutcome ~., family=binomial, data=train_df_SDOH)
summary(logr_model_SDOH)
# Training set results
pred_train_SDOH <- predict(logr_model_SDOH, newdata = train_df_SDOH, type="response")
# Change probabilities to 0, 1
pred_train_SDOH <- ifelse(pred_train_SDOH > 0.74, 1, 0)
# Test set results
pred_test_SDOH <- predict(logr_model_SDOH, newdata = test_df_SDOH, type="response")
# Change probabilities to 0, 1
pred_test_SDOH <- ifelse(pred_test_SDOH > 0.74, 1, 0)
# Converting to factors
train_actual_SDOH <- factor(train_df_SDOH$NegOutcome, levels = c(0, 1))
test_actual_SDOH <- factor(test_df_SDOH$NegOutcome, levels = c(0, 1))
train_pred_SDOH <- factor(pred_train_SDOH, levels = c(0, 1))
test_pred_SDOH <- factor(pred_test_SDOH, levels = c(0, 1))
# Building Confusion Matrices
results_SDOH_train <- confusionMatrix(train_actual_SDOH, train_pred_SDOH, positive = "1")
print(results_SDOH_train)
results_SDOH_test <- confusionMatrix(test_actual_SDOH, test_pred_SDOH, positive = "1")
print(results_SDOH_test)
pred_test_SDOH <- predict(logr_model_SDOH, newdata = test_df_SDOH, type="response")
# create ROC object
roc_object <- roc(test_df_SDOH$NegOutcome, pred_test_SDOH)
# calculate area under curve
auc(roc_object)
plot(roc_object, main = "ROC Curve", col = "#1c61b6", lwd = 2, print.thres=TRUE)
text(0.6, 0.3, paste("AUC =", round(auc(roc_object), 2)), col = "#1c61b6")
# code for optimal value -- fixed 80% sensitivity, how good is specificity
# test for different thresholds/lower thresholds
# Load in Libraries
library(CVrisk)
library(ggplot2)
library(dplyr)
library(haven)
library(tidyverse)
library(broom)
library(ggplot2)
library(caret)
library(tidymodels)
library(pROC)
# For Training
library(OpenML)
library(DT)
library(randomForest)
knitr::opts_knit$set(root.dir = '/Users/noreenmayat/Desktop/Senior Project')
## Visualizations for logistic regression and then different model (important variables, summary statistics, scores)
# EDA
# plots for continuous variables / covariances
# table 1 - avg value and std for each covariant; split between negative vs. non-negative outcomes in 2 columns, each row is a different covariant -- R table 1 package
# box plots
# distribution of the ASCVD score for negative vs non-negative outcomes -- observe density
# Load in Data
train_df <- read.csv("Cleaned Data/imputed_train.csv")
test_df <- read.csv("Cleaned Data/imputed_test.csv")
full_df <- read.csv("Cleaned Data/imputed_full.csv")
# Re-code as binaries for ML
train_df$Race1 <- ifelse(train_df$Race1 %in% c("white", "other"), 0, 1)
train_df$Sex <- ifelse(train_df$Sex == "male", 0, 1)
train_df$X <- NULL
test_df$Race1 <- ifelse(test_df$Race1 %in% c("white", "other"), 0, 1)
test_df$Sex <- ifelse(test_df$Sex == "male", 0, 1)
test_df$X <- NULL
full_df$Race1 <- ifelse(full_df$Race1 %in% c("white", "other"), 0, 1)
full_df$Sex <- ifelse(full_df$Sex == "male", 0, 1)
full_df$X <- NULL
# Compare Negative Outcomes by FastFood categories
barplot(table(full_df$NegOutcome, full_df$FastFood),
beside = T,
legend.text = T,
xlab = "Fast Food",
ylab = "Frequency",
main = "Negative CVD Outcome by Fast Food")
# Add a box around the plot
box()
# maybe use proportion instead of frequency
# boxplots -- for each variable for each category
# PCE
sum(is.na(train_df))
sum(is.na(test_df))
# Removing NAs
train_df <- na.omit(train_df)
test_df <- na.omit(test_df)
# Changing to factors
train_df$Race2 <- factor(train_df$Race2)
test_df$Race2 <- factor(test_df$Race2)
train_df <- train_df %>%
mutate(across(all_of(c('Ann_FamIncome', 'Ann_HHIncome', 'FastFood', 'BalancedMeals', 'LowCost', 'FoodSecurity')), ~factor(.x, levels = as.character(0:max(.x, na.rm = TRUE)), ordered = TRUE)))
test_df <- test_df %>%
mutate(across(all_of(c('Ann_FamIncome', 'Ann_HHIncome', 'FastFood', 'BalancedMeals', 'LowCost', 'FoodSecurity')), ~factor(.x, levels = as.character(0:max(.x, na.rm = TRUE)), ordered = TRUE)))
blueprint <- recipe(NegOutcome ~ ., data = train_df) %>%
step_center(all_of(c('Age', 'PovertyRatio', 'HDLChol', 'TotalChol', 'AvgSysBP'))) %>%
step_scale(all_of(c('Age', 'PovertyRatio', 'HDLChol', 'TotalChol', 'AvgSysBP')))
## Estimating blueprint parameters
blueprint_prep <- prep(blueprint, training = train_df)
## Transforming data
transformed_train <- bake(blueprint_prep, new_data = train_df)
transformed_test <- bake(blueprint_prep, new_data = test_df)
# Subset Appropriate Columns-- without SDOH, just PCE variables
train_df_PCE <- transformed_train[c('Age', 'Sex', 'Race1', 'HDLChol', 'TotalChol', 'AvgSysBP', 'BPMed', 'Diabetes', 'Smoking', 'NegOutcome')]
test_df_PCE <- transformed_test[c('Age', 'Sex', 'Race1', 'HDLChol', 'TotalChol', 'AvgSysBP', 'BPMed', 'Diabetes', 'Smoking', 'NegOutcome')]
# Subset Appropriate Columns-- with SDOH, and more stratified race variables
train_df_SDOH <- transformed_train[c('Age', 'Sex', 'Race2', 'HDLChol', 'TotalChol', 'AvgSysBP', 'BPMed', 'Diabetes', 'Smoking', 'Ann_FamIncome', 'Ann_HHIncome', 'PovertyRatio', 'FastFood', 'BalancedMeals', 'LowCost', 'FoodSecurity', 'NegOutcome')]
test_df_SDOH <- transformed_test[c('Age', 'Sex', 'Race2', 'HDLChol', 'TotalChol', 'AvgSysBP', 'BPMed', 'Diabetes', 'Smoking', 'Ann_FamIncome', 'Ann_HHIncome', 'PovertyRatio', 'FastFood', 'BalancedMeals', 'LowCost', 'FoodSecurity', 'NegOutcome')]
# Converting to factors
train_df_PCE$NegOutcome <- factor(train_df_PCE$NegOutcome, levels = c(0, 1))
test_df_PCE$NegOutcome <- factor(test_df_PCE$NegOutcome, levels = c(0, 1))
# Converting to factors
train_df_SDOH$NegOutcome <- factor(train_df_SDOH$NegOutcome, levels = c(0, 1))
test_df_SDOH$NegOutcome <- factor(test_df_SDOH$NegOutcome, levels = c(0, 1))
logr_model_PCE <- glm(NegOutcome ~., family=binomial, data=train_df_PCE)
summary(logr_model_PCE)
# Training set results
pred_train_PCE <- predict(logr_model_PCE, newdata = train_df_PCE, type="response")
# Change probabilities to 0, 1
pred_train_PCE <- ifelse(pred_train_PCE > 0.65, 1, 0)
# Test set results
pred_test_PCE <- predict(logr_model_PCE, newdata = test_df_PCE, type="response")
# Change probabilities to 0, 1
pred_test_PCE <- ifelse(pred_test_PCE > 0.65, 1, 0)
train_pred_PCE <- factor(pred_train_PCE, levels = c(0, 1))
test_pred_PCE <- factor(pred_test_PCE, levels = c(0, 1))
# Building Confusion Matrices
results_PCE_train <- confusionMatrix(train_df_PCE$NegOutcome, train_pred_PCE, positive = "1")
print(results_PCE_train)
results_PCE_test <- confusionMatrix(test_df_PCE$NegOutcome, test_pred_PCE, positive = "1")
print(results_PCE_test)
pred_test_PCE <- predict(logr_model_PCE, newdata = test_df_PCE, type="response")
# create ROC object
roc_object <- roc(test_df_PCE$NegOutcome, pred_test_PCE)
# calculate area under curve
auc(roc_object)
plot(roc_object, main = "ROC Curve", col = "#1c61b6", lwd = 2, print.thres=TRUE)
text(0.6, 0.3, paste("AUC =", round(auc(roc_object), 2)), col = "#1c61b6")
# code for optimal threshold value
logr_model_SDOH <- glm(NegOutcome ~., family=binomial, data=train_df_SDOH)
summary(logr_model_SDOH)
# Training set results
pred_train_SDOH <- predict(logr_model_SDOH, newdata = train_df_SDOH, type="response")
# Change probabilities to 0, 1
pred_train_SDOH <- ifelse(pred_train_SDOH > 0.74, 1, 0)
# Test set results
pred_test_SDOH <- predict(logr_model_SDOH, newdata = test_df_SDOH, type="response")
# Change probabilities to 0, 1
pred_test_SDOH <- ifelse(pred_test_SDOH > 0.74, 1, 0)
train_pred_SDOH <- factor(pred_train_SDOH, levels = c(0, 1))
test_pred_SDOH <- factor(pred_test_SDOH, levels = c(0, 1))
# Building Confusion Matrices
results_SDOH_train <- confusionMatrix(train_df_SDOH$NegOutcome, train_pred_SDOH, positive = "1")
print(results_SDOH_train)
results_SDOH_test <- confusionMatrix(test_df_SDOH$NegOutcome, test_pred_SDOH, positive = "1")
print(results_SDOH_test)
pred_test_SDOH <- predict(logr_model_SDOH, newdata = test_df_SDOH, type="response")
# create ROC object
roc_object <- roc(test_df_SDOH$NegOutcome, pred_test_SDOH)
# calculate area under curve
auc(roc_object)
plot(roc_object, main = "ROC Curve", col = "#1c61b6", lwd = 2, print.thres=TRUE)
text(0.6, 0.3, paste("AUC =", round(auc(roc_object), 2)), col = "#1c61b6")
# code for optimal value -- fixed 80% sensitivity, how good is specificity
# test for different thresholds/lower thresholds
# Adjust factor levels
levels(train_df_PCE$NegOutcome) <- make.names(levels(train_df_PCE$NegOutcome))
levels(test_df_PCE$NegOutcome) <- make.names(levels(test_df_PCE$NegOutcome))
initial_rf <- randomForest(NegOutcome ~., data = train_df_PCE)
plot(initial_rf)
resample <- trainControl(method = "cv",
number = 5,
classProbs = TRUE,
summaryFunction = twoClassSummary)
hyper_grid <- expand.grid(mtry = c(3, 4, 5, 7, 8, 9),
splitrule = c("gini", "extratrees"),
min.node.size = c(5, 7, 9))
rf_fit <- train(NegOutcome ~ .,
data = train_df_PCE,
method = "ranger",
verbose = FALSE,
trControl = resample,
tuneGrid = hyper_grid,
metric = "ROC",
num.trees = 470)
# Adjust factor levels
levels(train_df_SDOH$NegOutcome) <- make.names(levels(train_df_SDOH$NegOutcome))
levels(test_df_SDOH$NegOutcome) <- make.names(levels(test_df_SDOH$NegOutcome))
initial_rf <- randomForest(NegOutcome ~., data = train_df_PCE)
plot(initial_rf)
# Final Model
fitControl_final <- trainControl(method = "none", classProbs = TRUE)
RF_final <- train(NegOutcome ~.,
data = train_df_SDOH,
method = "ranger",
trControl = fitControl_final,
metric = "ROC",
verbose = FALSE,
tuneGrid = data.frame(mtry = 3,
min.node.size = 9,
splitrule = "extratrees"),
num.trees = 470)
# Training set results
RF_pred_train <- predict(RF_final, newdata = train_df_SDOH)
RF_train_results <- confusionMatrix(train_df_SDOH$NegOutcome, RF_pred_train)
print(RF_train_results)
# Test set results
RF_pred_test <- predict(RF_final, newdata = test_df_SDOH)
RF_test_results <- confusionMatrix(test_df_SDOH$NegOutcome, RF_pred_test)
print(RF_test_results)
set.seed(123)
resample <- trainControl(method = "cv",
number = 5,
classProbs = TRUE,
summaryFunction = twoClassSummary)
hyper_grid <- expand.grid(mtry = c(3, 4, 5, 7, 8, 9),
splitrule = c("gini", "extratrees"),
min.node.size = c(5, 7, 9))
rf_fit <- train(NegOutcome ~ .,
data = train_df_PCE,
method = "ranger",
verbose = FALSE,
trControl = resample,
tuneGrid = hyper_grid,
metric = "ROC",
num.trees = 470)
